---
title: "Model training"
output: html_notebook
---

This notebook trains a classification model which distinguishes between actual quotations to the biblical text and mere noise. It does not attempt to distinguish between versions of a biblical text: that kind of cleaning will happen later.

We are going to use the parsnip package and its attendants to train and evaluate different models, then pick the best one.

```{r setup, message=FALSE}
library(tidyverse)
library(tidymodels)
library(parsnip)
library(dials)
library(keras)
library(furrr)
```

The training data is located in the database. It is stored as a table contained document (i.e., newspaper) and verse IDs, with a boolean labeling them as a genuine or false match. That match indicates whether the verse was indeed quoted, but not whether that specific version was quoted. Another table contains measurements of the features of the potential quotation. We join the labeled data to those measurements. But it might also be useful to know certain information about the version of the verse. For instance, the Book of Mormon reproduces a number of verses or phrases from the KJV and so it has a much higher rate of potential matches that measure highly but aren't actually matches. So we pull in some version information that we will manipulate. This includes a table measuring how similar verses are to one another within the same version, which is a way of knowing whether a verse is unique or not. Finally we split the data into training and testing sets. The testing set is inviolable, and will be used for model validation later. To make sure the data is available for inspection later, we will only do that if the data has not been written to disk; otherwise, we will load the data from disk.

```{r}
# Check if we have already commited the training and validation data
if (!file.exists("apb-training.csv") |
    !file.exists("apb-testing.csv") |
    !file.exists("apb-labeled-quotations.csv")) {
  
  message("Reading the labels from the database and creating train/test split.\n")
  
  # Get the data from the database and manipulate it
  library(odbc)
  db <- dbConnect(odbc::odbc(), "Research DB")
  apb_labeled <- tbl(db, "apb_labeled")
  apb_potential_quotations <- tbl(db, "apb_potential_quotations")
  scriptures <- tbl(db, "scriptures") %>% select(verse_id = doc_id, version)
  scripture_sim <- tbl(db, "scriptures_intraversion_similarity") 
  labeled_quotations <- apb_labeled %>% 
    left_join(apb_potential_quotations, by = c("verse_id", "doc_id")) %>% 
    left_join(scriptures, by = c("verse_id")) %>% 
    left_join(scripture_sim, by = c("verse_id")) %>% 
    collect() %>% 
    filter(!is.na(tokens),
           version != "Book of Mormon",
           version != "Pearl of Great Price",
           version != "Doctrine and Covenants") %>% 
    mutate(match = if_else(match, "quotation", "noise"),
           match = factor(match, levels = c("quotation", "noise"))) %>% 
    mutate(runs_pval = if_else(is.na(runs_pval), 1, runs_pval)) %>% 
    select(-version)
  
  # Split the labeled data into training and validation sets
  set.seed(1989)
  data_split <- initial_split(labeled_quotations, strata = "match", p = 0.85)
  training <- training(data_split)
  testing  <- testing(data_split)
  write_csv(labeled_quotations, "apb-labeled-quotations.csv")
  write_csv(training, "apb-training.csv")
  write_csv(testing, "apb-testing.csv")
  
  # Cleanup
  dbDisconnect(db)
  rm(data_split)
  rm(apb_labeled)
  rm(apb_potential_quotations)
  rm(scriptures)
  rm(scripture_sim)
  rm(db)
  
} else {
  
  message("The training or testing data already exists. Loading from disk.\n")
  spec <- cols(verse_id = col_character(),
               doc_id = col_character(),
               match = readr::col_factor(levels = c("quotation", "noise")),
               tokens = col_integer(),
               tfidf = col_double(),
               proportion = col_double(),
               runs_pval = col_double(), 
               sim_total = col_double(), 
               sim_mean = col_double())
  labeled_quotations <- read_csv("apb-labeled-quotations.csv", col_types = spec)
  training <- read_csv("apb-training.csv", col_types = spec)
  testing <- read_csv("apb-testing.csv", col_types = spec)
  rm(spec)
}
```

We are going to remove the `verse_id` and `doc_id` columns because they are not predictor or response variables. 

```{r}
labeled_quotations <- labeled_quotations %>% select(-verse_id, -doc_id)
training <- training %>% select(-verse_id, -doc_id)
testing <- testing %>% select(-verse_id, -doc_id)
```

Some brief exploration of the data confirms that there is a clear separation in the data.

```{r}
labeled_quotations %>% 
  group_by(match) %>% 
  summarize(n(), mean(tokens), mean(tfidf),
            mean(proportion), mean(runs_pval),
            mean(sim_total), mean(sim_mean)) %>% 
  gather("measurement", "value", -match) %>% 
  mutate(value = round(value, 2)) %>% 
  spread(match, value)
```

We can also see the separation in the data, thought it is not as clear as we would like.

```{r}
ggplot(labeled_quotations, aes(tokens, tfidf, color = match)) +
  geom_jitter(shape = 1) +
  theme_classic() +
  coord_cartesian(xlim = c(0, 100), ylim = c(0, 12)) +
  labs(title = "Comparison of genuine quotations versus noise")
```

We are going to pre-process the data to center and scale the predictors.

```{r}
data_recipe <- recipe(match ~ ., data = training) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), -match) %>% 
  prep(training = training, retain = TRUE)

training_normalized = bake(data_recipe, newdata = training)
testing_normalized = bake(data_recipe, newdata = testing)
```

We aren't entirely sure which predictors would be best. So we want to define different formulas for different sets of predictors, along with a dials parameter to generated the tuning grid.

```{r}
predictors_formulas <- list(
  # Throw all the information
  all = match ~ .,
  # Just the core numeric predictors
  core = match ~ tokens + tfidf + proportion,
  # The core numeric predictors plus runs_pval
  numeric = match ~ tokens + tfidf + proportion + runs_pval,
  # The core numeric predictors plus runs_pval with log of tokens
  numeric_log = match ~ log(tokens) + tfidf + proportion + runs_pval,
  # The core numeric predictors with interaction between tokens and tfidf
  interaction = match ~ tokens * tfidf + proportion + runs_pval,
  # The numeric predictors plus sim_total and lds
  sim1 = match ~ tokens + tfidf + proportion + runs_pval + sim_total + lds_not.lds,
  # The numeric predictors plus sim_mean and lds
  sim2 = match ~ tokens + tfidf + proportion + runs_pval + sim_mean + lds_not.lds,
  # The numeric predictors plus sim_total but not lds
  sim3 = match ~ tokens + tfidf + proportion + runs_pval + sim_total,
  # The numeric predictors plus sim_total but not lds
  sim4 = match ~ tokens + tfidf + proportion + runs_pval + sim_mean
)

# A dials object for setting the predictors parameter
predictors <- new_qual_param("character", 
                             values = names(predictors_formulas),
                             label = c(predictors = "Predictors"))
```

We will begin by training a logistic regression model to classify the quotations. We will create a tuning grid with the different predictors, parameters specific to the logistic regression model. We will then train the models on those parameters, and measure the accuracy according to various metrics that use the confusion matrix. The goal is to end up with a data frame with the parameters, models, and accuracy measures.

```{r, warning=FALSE}
set.seed(7260)
logistic_spec <- logistic_reg(mode = "classification", 
                              regularization = varying(),
                              mixture = varying())

logistic_params <- grid_regular(regularization,
                                mixture,
                                predictors,
                                levels = 5)
logistic_params$model_type <- "logistic regression"
logistic_params$accuracy <- vector("list", nrow(logistic_params))
logistic_models <- vector("list", nrow(logistic_params))

for (i in 1:nrow(logistic_params)) {
  logistic_models[[i]] <- logistic_spec %>% 
    merge(logistic_params[i, ]) %>% 
    parsnip::fit(predictors_formulas[[logistic_params[[i, "predictors"]]]],
                 data = training_normalized,
                 engine = "glm")
}

# Calculate the confusion matrix and the resulting accuracy measures
predictions <- function(model, type = c("training", "testing")) {
  type <- match.arg(type)
  df <- switch(type,
               training = training_normalized,
               testing = testing_normalized)
  df %>% 
    select(match) %>% 
    mutate(pred_class = model %>% 
             predict_class(df),
           pred_probs = model %>% 
             predict_classprob(df) %>% 
             pull(quotation))
}
accuracy_measures <- function(model, type = c("training", "testing")) {
  type <- match.arg(type)
  model %>% 
    predictions(type = type) %>%
    conf_mat(truth = match, estimate = pred_class) %>%
    summary()
}

for (i in seq_along(logistic_models)) {
  logistic_params$accuracy[[i]] <- accuracy_measures(logistic_models[[i]])
}

logistic_params <- logistic_params %>% 
  unnest(accuracy) %>% 
  spread(name, value)

logistic_params %>% 
  arrange(desc(balanced_accuracy)) %>% 
  select(balanced_accuracy, precision, recall, everything()) %>%
  head(5) 
```

Having done that for logistic regression, we can also do it for boosted trees. This is not full done yet. These models drastically overfit.

```{r, warning=FALSE}
set.seed(275)
bt_spec <- boost_tree(mode = "classification", 
                        # mtry = varying(),
                        trees = varying(),
                        # min_n = varying(),
                        tree_depth = varying(),
                        # learn_rate = varying(),
                        # loss_reduction = varying(),
                        # sample_size = varying()
                      )

bt_params <- grid_random(predictors,
                         # mtry,
                         trees,
                         # min_n,
                         tree_depth,
                         # learn_rate,
                         # loss_reduction,
                         # sample_size,
                         size = 30)

bt_params$model_type <- "boosted tree"
bt_params$accuracy <- vector("list", nrow(bt_params))

train_bt <- function(i) {
  bt_spec %>% 
    merge(bt_params[i, ]) %>% 
    parsnip::fit(predictors_formulas[[bt_params[[i, "predictors"]]]],
                 data = training_normalized,
                 engine = "xgboost")
}

bt_models <- seq_len(nrow(bt_params)) %>% map(train_bt)

for (i in seq_along(bt_models)) {
  bt_params$accuracy[[i]] <- accuracy_measures(bt_models[[i]])
}

bt_params <- bt_params %>% 
  unnest(accuracy) %>% 
  spread(name, value)

bt_params %>% 
  arrange(desc(balanced_accuracy)) %>% 
  select(balanced_accuracy, precision, recall, everything()) %>%
  head(5) 
```

Having done that for logistic regression and boosted trees, we want to do the same thing for a neural net.

```{r, warning=FALSE}
set.seed(87026)
nnet_spec <- mlp(mode = "classification", 
                 hidden_units = varying(),
                 regularization = varying(),
                 dropout = varying(),
                 activation = varying(),
                 epochs = 500,
                 others = list(verbose = 1,
                               validation_split = 0.2))

nnet_params <- grid_random(predictors,
                           hidden_units,
                           regularization,
                           dropout,
                           activation,
                           size = 2)
nnet_params$model_type <- "nnet keras"
nnet_params$accuracy <- vector("list", nrow(nnet_params))

train_nnet <- function(i) {
  nnet_spec %>% 
    merge(nnet_params[i, ]) %>% 
    parsnip::fit(predictors_formulas[[nnet_params[[i, "predictors"]]]],
                 data = training_normalized,
                 engine = "keras")
}

nnet_models <- seq_len(nrow(nnet_params)) %>% map(train_nnet)

for (i in seq_along(nnet_models)) {
  nnet_params$accuracy[[i]] <- accuracy_measures(nnet_models[[i]])
}

nnet_params <- nnet_params %>% 
  unnest(accuracy) %>% 
  spread(name, value)

nnet_params %>% 
  arrange(desc(balanced_accuracy)) %>% 
  select(balanced_accuracy, precision, recall, everything()) %>%
  head(5) 
```

Now we can select the best model from the several types we have created.

```{r}
bind_rows(
  nnet_params,
  logistic_params
) %>% 
  select(model_type, balanced_accuracy, precision, recall, sens, spec, predictors, everything()) %>% 
  arrange(desc(precision)) %>% 
  head(10)
```

<!-- We can then evaluate the accuracy of the model on the training dataset. (Later we will evaluate against the testing dataset.) -->

```{r}
# training_results <- training_normalized %>%
#   select(match) %>%
#   mutate(pred_class = model_fit %>%
#            predict_class(training_normalized),
#          pred_probs = model_fit %>%
#            predict_classprob(training_normalized) %>%
#            pull(quotation))
# training_results %>% accuracy(truth = match, estimate = pred_class)
# training_results %>% roc_auc(truth = match, estimate = pred_probs)
# training_results %>% pr_auc(truth = match, estimate = pred_probs)
# training_results %>% conf_mat(truth = match, estimate = pred_class)
# training_results %>% conf_mat(truth = match, estimate = pred_class) %>% summary()
# training_results %>% roc_curve(match, pred_probs) %>%
#   ggplot(aes(x = 1 - specificity, y = sensitivity)) +
#   geom_path() +
#   geom_abline(lty = 3) +
#   coord_equal() +
#   theme_classic() +
#   labs(title = "ROC curve")
```

